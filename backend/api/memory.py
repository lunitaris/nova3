from fastapi import APIRouter, HTTPException, Depends, Query
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import logging
import csv
import io
from enum import Enum
from datetime import datetime
import os
import json

from backend.config import config
from backend.memory.synthetic_memory import synthetic_memory
from backend.memory.vector_store import vector_store
from backend.memory.symbolic_memory import symbolic_memory
import networkx as nx
from backend.memory.enhanced_symbolic_memory import enhanced_symbolic_memory
from fastapi import Body
from backend.utils.profiler import profile
from backend.memory.graph_postprocessor import postprocess_graph



logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/memory", tags=["memory"])

# Mod√®les de donn√©es
class MemoryItem(BaseModel):
    content: str
    topic: Optional[str] = "general"
    metadata: Optional[Dict[str, Any]] = None
    score_pertinence: Optional[float] = None
    source_conversation_id: Optional[str] = None

class MemoryResponse(BaseModel):
    memory_id: Optional[int] = None
    status: str
    message: str
    error: Optional[str] = None

class SearchQuery(BaseModel):
    query: str
    topic: Optional[str] = None
    max_results: Optional[int] = 5
    min_score: Optional[float] = 0.0
    max_age_days: Optional[int] = None

class MemorySearchResult(BaseModel):
    results: List[Dict[str, Any]]
    count: int

class SortField(str, Enum):
    date = "date"
    score = "score"
    topic = "topic"
    relevance = "relevance"

class SortOrder(str, Enum):
    asc = "asc"
    desc = "desc"

class MemoryAuditQuery(BaseModel):
    include_deleted: Optional[bool] = False
    include_expired: Optional[bool] = False
    memory_type: Optional[str] = "all"  # "vector", "symbol", "all"
    sort_by: Optional[SortField] = SortField.date
    sort_order: Optional[SortOrder] = SortOrder.desc
    topic: Optional[str] = None
    min_confidence: Optional[float] = 0.0
    format: Optional[str] = "json"  # "json", "csv"



# Ajouter dans la classe de configuration (MemoryConfig)
class MemoryConfig:
    vector_dimension: int = 1536
    max_history_length: int = 50
    synthetic_memory_refresh_interval: int = 10
    use_chatgpt_for_symbolic_memory: bool = False  # Nouveau param√®tre




# Routes existantes

@router.post("/remember", response_model=MemoryResponse)
async def remember_information(item: MemoryItem):
    """
    M√©morise explicitement une information.
    """
    try:
        memory_id = vector_store.add_memory(
            content=item.content,
            metadata={"topic": item.topic, **(item.metadata or {})},
            score_pertinence=item.score_pertinence,
            source_conversation_id=item.source_conversation_id
        )
        
        if memory_id >= 0:
            return MemoryResponse(
                memory_id=memory_id,
                status="success",
                message="Information m√©moris√©e avec succ√®s"
            )
        else:
            return MemoryResponse(
                status="error",
                message="√âchec de la m√©morisation",
                error="Erreur interne lors de la m√©morisation"
            )
    
    except Exception as e:
        logger.error(f"Erreur lors de la m√©morisation: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")



@router.post("/search", response_model=MemorySearchResult)
@profile("memory_vector_search")
async def search_memories(query: SearchQuery):
    """
    Recherche des informations dans la m√©moire.
    """
    try:
        results = vector_store.search_memories(
            query=query.query,
            k=query.max_results,
            min_score=query.min_score,
            max_age_days=query.max_age_days
        )
        
        # Filtrer par sujet si sp√©cifi√©
        if query.topic:
            results = [r for r in results if r.get("topic") == query.topic]
        
        return MemorySearchResult(
            results=results,
            count=len(results)
        )
    
    except Exception as e:
        logger.error(f"Erreur lors de la recherche en m√©moire: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

@router.get("/topics", response_model=List[str])
async def list_memory_topics():
    """
    Liste tous les sujets disponibles dans la m√©moire.
    """
    try:
        topics = list(synthetic_memory.memory_data.get("topics", {}).keys())
        return topics
    
    except Exception as e:
        logger.error(f"Erreur lors de la liste des sujets: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

@router.get("/topic/{topic}", response_model=List[Dict[str, Any]])
async def get_topic_memories(topic: str):
    """
    R√©cup√®re toutes les m√©moires d'un sujet sp√©cifique.
    """
    try:
        memories = synthetic_memory.get_memory_by_topic(topic)
        return memories
    
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des m√©moires du sujet {topic}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

@router.post("/compress", response_model=MemoryResponse)
async def compress_memories():
    """
    Lance une compression manuelle des m√©moires synth√©tiques.
    """
    try:
        success = await synthetic_memory.compress_memory()
        
        if success:
            return MemoryResponse(
                status="success",
                message="M√©moires compress√©es avec succ√®s"
            )
        else:
            return MemoryResponse(
                status="error",
                message="√âchec de la compression des m√©moires",
                error="Erreur interne lors de la compression"
            )
    
    except Exception as e:
        logger.error(f"Erreur lors de la compression des m√©moires: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

@router.delete("/memory/{memory_id}", response_model=MemoryResponse)
async def delete_memory(memory_id: str):
    """
    Supprime une m√©moire sp√©cifique.
    """
    try:
        success = vector_store.delete_memory(memory_id)
        
        if success:
            return MemoryResponse(
                status="success",
                message=f"M√©moire {memory_id} supprim√©e avec succ√®s"
            )
        else:
            return MemoryResponse(
                status="error",
                message=f"√âchec de la suppression de la m√©moire {memory_id}",
                error="M√©moire non trouv√©e ou erreur interne"
            )
    
    except Exception as e:
        logger.error(f"Erreur lors de la suppression de la m√©moire {memory_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

@router.put("/memory/{memory_id}", response_model=MemoryResponse)
async def update_memory(memory_id: str, item: MemoryItem):
    """
    Met √† jour une m√©moire existante.
    """
    try:
        success = vector_store.update_memory(
            memory_id=memory_id,
            content=item.content,
            metadata={"topic": item.topic, **(item.metadata or {})},
            score_pertinence=item.score_pertinence
        )
        
        if success:
            return MemoryResponse(
                status="success",
                message=f"M√©moire {memory_id} mise √† jour avec succ√®s"
            )
        else:
            return MemoryResponse(
                status="error",
                message=f"√âchec de la mise √† jour de la m√©moire {memory_id}",
                error="M√©moire non trouv√©e ou erreur interne"
            )
    
    except Exception as e:
        logger.error(f"Erreur lors de la mise √† jour de la m√©moire {memory_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

# Nouvelle route d'audit pour la m√©moire
@router.get("/audit")
async def audit_memories(
    include_deleted: bool = Query(False, description="Inclure les souvenirs supprim√©s"),
    include_expired: bool = Query(False, description="Inclure les souvenirs expir√©s"),
    memory_type: str = Query("all", description="Type de m√©moire: vector, symbol, all"),
    sort_by: SortField = Query(SortField.date, description="Champ de tri"),
    sort_order: SortOrder = Query(SortOrder.desc, description="Ordre de tri"),
    topic: Optional[str] = Query(None, description="Filtrer par sujet"),
    min_confidence: float = Query(0.0, description="Score de confiance minimal"),
    format: str = Query("json", description="Format de sortie: json, csv")
):
    """
    R√©cup√®re tous les souvenirs pour audit et analyse.
    Permet de filtrer, trier et exporter les donn√©es de m√©moire.
    """
    try:
        all_memories = []
        
        # R√©cup√©rer les m√©moires vectorielles si demand√©
        if memory_type in ["vector", "all"]:
            vector_memories = vector_store.get_all_memories(include_deleted=include_deleted)
            for memory in vector_memories:
                # Ajouter le type de m√©moire pour diff√©renciation
                memory["memory_type"] = "vector"
                
                # Filtrer par sujet si demand√©
                if topic and memory.get("topic") != topic:
                    continue
                    
                # Filtrer par score de confiance
                if memory.get("score_pertinence", 0) < min_confidence:
                    continue
                    
                all_memories.append(memory)
        
        # R√©cup√©rer les entit√©s symboliques si demand√©
        if memory_type in ["symbol", "all"]:
            # Entit√©s
            symbolic_entities = symbolic_memory.get_all_entities(include_expired=include_expired)
            for entity in symbolic_entities:
                # Convertir l'entit√© au format m√©moire pour l'audit
                memory_entry = {
                    "memory_type": "symbolic_entity",
                    "entity_id": entity.get("entity_id"),
                    "content": f"Entit√©: {entity.get('name')} (Type: {entity.get('type')})",
                    "timestamp": entity.get("last_updated"),
                    "confidence": entity.get("confidence", 0),
                    "valid_from": entity.get("valid_from"),
                    "valid_to": entity.get("valid_to"),
                    "attributes": entity.get("attributes"),
                    "name": entity.get("name"),
                    "type": entity.get("type")
                }
                
                # Filtrer par score de confiance
                if memory_entry.get("confidence", 0) < min_confidence:
                    continue
                    
                all_memories.append(memory_entry)
            
            # Relations
            symbolic_relations = symbolic_memory.get_all_relations(include_expired=include_expired)
            for relation in symbolic_relations:
                # Convertir la relation au format m√©moire pour l'audit
                memory_entry = {
                    "memory_type": "symbolic_relation",
                    "content": f"Relation: {relation.get('source_name')} - {relation.get('relation')} -> {relation.get('target_name')}",
                    "timestamp": relation.get("timestamp"),
                    "confidence": relation.get("confidence", 0),
                    "valid_from": relation.get("valid_from"),
                    "valid_to": relation.get("valid_to"),
                    "source": relation.get("source"),
                    "relation": relation.get("relation"),
                    "target": relation.get("target"),
                    "source_name": relation.get("source_name"),
                    "target_name": relation.get("target_name")
                }
                
                # Filtrer par score de confiance
                if memory_entry.get("confidence", 0) < min_confidence:
                    continue
                    
                all_memories.append(memory_entry)
        
        # Trier les r√©sultats
        sort_key = None
        if sort_by == SortField.date:
            sort_key = lambda x: x.get("timestamp", "")
        elif sort_by == SortField.score:
            sort_key = lambda x: x.get("score_pertinence", x.get("confidence", 0))
        elif sort_by == SortField.topic:
            sort_key = lambda x: x.get("topic", "")
        elif sort_by == SortField.relevance:
            sort_key = lambda x: (x.get("score_pertinence", 0), x.get("confidence", 0))
        
        if sort_key:
            reverse = sort_order == SortOrder.desc
            all_memories.sort(key=sort_key, reverse=reverse)
        
        # Retourner au format demand√©
        if format.lower() == "csv":
            # Pr√©parer le CSV
            output = io.StringIO()
            
            # D√©terminer les champs du CSV
            fieldnames = set()
            for memory in all_memories:
                fieldnames.update(memory.keys())
            fieldnames = sorted(list(fieldnames))
            
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()
            
            for memory in all_memories:
                writer.writerow(memory)
            
            csv_content = output.getvalue()
            output.close()
            
            # Retourner comme fichier CSV √† t√©l√©charger
            headers = {
                "Content-Disposition": f"attachment; filename=memory_audit_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            }
            return JSONResponse(
                content=csv_content,
                media_type="text/csv",
                headers=headers
            )
        else:
            # Format JSON par d√©faut
            return {
                "memories": all_memories,
                "count": len(all_memories),
                "filters": {
                    "include_deleted": include_deleted,
                    "include_expired": include_expired,
                    "memory_type": memory_type,
                    "sort_by": sort_by,
                    "sort_order": sort_order,
                    "topic": topic,
                    "min_confidence": min_confidence
                }
            }
    
    except Exception as e:
        logger.error(f"Erreur lors de l'audit des m√©moires: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

# Route pour l'historique d'une entit√© (pour la roadmap √©tape 3)
@router.get("/timeline/{entity_id}")
async def get_entity_timeline(entity_id: str):
    """
    R√©cup√®re l'historique complet d'une entit√© symbolique.
    """
    try:
        history = symbolic_memory.get_entity_history(entity_id)
        
        if not history:
            raise HTTPException(status_code=404, detail=f"Entit√© {entity_id} non trouv√©e ou sans historique")
            
        return {
            "entity_id": entity_id,
            "history": history,
            "count": len(history)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration de l'historique: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")




############################## API FOR SYMBLIC GRAPH #########################################

@router.get("/graph")
async def get_memory_graph(
    format: str = Query("d3", description="Format de sortie: d3, cytoscape"),
    include_expired: bool = Query(False, description="Inclure les entit√©s supprim√©es ou expir√©es"),
    conversation_id: Optional[str] = Query(None, description="ID de conversation pour filtrage")
):
    """
    R√©cup√®re le graphe de connaissances symbolique dans un format adapt√© √† la visualisation.
    Si conversation_id est fourni, filtre les entit√©s li√©es √† cette conversation.
    """
    try:
        # Cr√©er un graphe NetworkX
        G = nx.DiGraph()
        
        # R√©cup√©rer toutes les entit√©s et relations
        entities = symbolic_memory.get_all_entities(include_expired=include_expired)
        relations = symbolic_memory.get_all_relations(include_expired=include_expired)
        
        # Si un ID de conversation est fourni, on pourrait filtrer les entit√©s
        # Ceci est un emplacement pour une future impl√©mentation de filtrage
        # Pour l'instant, nous renvoyons le graphe complet dans tous les cas
        if conversation_id:
            # En attente d'une impl√©mentation future qui associe les entit√©s aux conversations
            # Pour l'instant, inclure toutes les entit√©s quel que soit l'ID de conversation
            logger.info(f"Filtrage par conversation demand√© pour {conversation_id}, mais non impl√©ment√©")
            # Future impl√©mentation de filtrage ici
            pass
        
        # Ajouter les entit√©s comme noeuds
        for entity in entities:
            entity_id = entity.get("entity_id")
            
            # Propri√©t√©s du noeud
            node_props = {
                "id": entity_id,
                "label": entity.get("name", "Entit√© sans nom"),
                "type": entity.get("type", "unknown"),
                "attributes": entity.get("attributes", {}),
                "confidence": entity.get("confidence", 0.0),
                "group": _get_node_group(entity.get("type", "unknown"))
            }
            
            G.add_node(entity_id, **node_props)
        
        # Ajouter les relations comme liens
        for relation in relations:
            source = relation.get("source")
            target = relation.get("target")
            
            # V√©rifier que les noeuds existent
            if source in G.nodes and target in G.nodes:
                # Propri√©t√©s du lien
                edge_props = {
                    "id": f"{source}_{relation.get('relation')}_{target}",
                    "label": relation.get("relation", "lien"),
                    "confidence": relation.get("confidence", 0.0)
                }
                
                G.add_edge(source, target, **edge_props)
        
        # Formater selon le format demand√©
        if format == "d3":
            result = _format_graph_d3(G)
        elif format == "cytoscape":
            result = _format_graph_cytoscape(G)
        else:
            result = _format_graph_d3(G)  # D3 par d√©faut
        
        return result
    
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration du graphe: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")


def _get_node_group(entity_type: str) -> int:
    """
    Attribue un groupe (utilis√© pour la couleur) selon le type d'entit√©.
    """
    type_groups = {
        "person": 1,
        "place": 2,
        "date": 3,
        "concept": 4,
        "preference": 5,
        "profession": 6,
        "contact": 7,
        "device": 8,
        "user": 0  # Utilisateurs en groupe sp√©cial
    }
    
    return type_groups.get(entity_type.lower(), 9)  # 9 = autre type

def _format_graph_d3(G: nx.DiGraph) -> Dict[str, Any]:
    """
    Formate le graphe pour D3.js (format force-directed graph).
    """
    # Convertir le graphe en format attendu par D3.js
    nodes = []
    links = []
    
    for node_id, node_data in G.nodes(data=True):
        nodes.append({
            "id": node_id,
            "name": node_data.get("label", node_id),
            "group": node_data.get("group", 1),
            "type": node_data.get("type", "unknown"),
            "confidence": node_data.get("confidence", 0.0)
        })
    
    for source, target, edge_data in G.edges(data=True):
        links.append({
            "source": source,
            "target": target,
            "label": edge_data.get("label", "lien"),
            "value": edge_data.get("confidence", 0.5) * 2,  # √âpaisseur proportionnelle √† la confiance
            "confidence": edge_data.get("confidence", 0.5)
        })
    
    return {"nodes": nodes, "links": links}

def _format_graph_cytoscape(G: nx.DiGraph) -> Dict[str, Any]:
    """
    Formate le graphe pour Cytoscape.js.
    """
    elements = []
    
    # Nodes
    for node_id, node_data in G.nodes(data=True):
        elements.append({
            "data": {
                "id": node_id,
                "label": node_data.get("label", node_id),
                "group": node_data.get("group", 1),
                "type": node_data.get("type", "unknown"),
                "confidence": node_data.get("confidence", 0.0)
            }
        })
    
    # Edges
    for source, target, edge_data in G.edges(data=True):
        elements.append({
            "data": {
                "id": edge_data.get("id", f"{source}-{target}"),
                "source": source,
                "target": target,
                "label": edge_data.get("label", "lien"),
                "weight": edge_data.get("confidence", 0.5),
                "confidence": edge_data.get("confidence", 0.5)
            }
        })
    
    return {"elements": elements}


@router.post("/update_symbolic_graph", response_model=Dict[str, Any])
async def update_symbolic_graph(text: str = Body(...), confidence: float = Body(0.7)):
    """
    Met √† jour le graphe symbolique √† partir d'un texte.
    Utilise ChatGPT si activ√© dans la configuration, sinon utilise l'extracteur local.
    """
    try:
        # Utiliser la version am√©lior√©e qui choisit automatiquement entre ChatGPT et local
        result = await enhanced_symbolic_memory.update_graph_from_text(
            text=text,
            confidence=confidence
        )
        
        return {
            "status": "success",
            "result": result
        }
    
    except Exception as e:
        logger.error(f"Erreur lors de la mise √† jour du graphe symbolique: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")





@router.get("/symbolic_extraction_config")
async def get_symbolic_extraction_config():
    """
    R√©cup√®re la configuration actuelle de l'extraction symbolique.
    """
    try:
        # Version plus robuste pour v√©rifier la configuration
        use_chatgpt = False
        try:
            if hasattr(config, "memory") and hasattr(config.memory, "use_chatgpt_for_symbolic_memory"):
                use_chatgpt = config.memory.use_chatgpt_for_symbolic_memory
        except Exception as config_error:
            logger.error(f"Erreur lors de l'acc√®s √† la configuration: {str(config_error)}")
        
        # V√©rification de la cl√© API avec gestion d'erreurs
        has_api_key = False
        try:
            has_api_key = bool(enhanced_symbolic_memory.openai_api_key)
        except Exception as key_error:
            logger.error(f"Erreur lors de la v√©rification de la cl√© API: {str(key_error)}")
        
        return {
            "use_chatgpt": use_chatgpt,
            "has_api_key": has_api_key,
            "extraction_available": use_chatgpt and has_api_key
        }
    except Exception as e:
        # Log d√©taill√© de l'erreur
        logger.error(f"Erreur lors de la r√©cup√©ration de la configuration: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # Renvoyer une r√©ponse de secours plut√¥t qu'une erreur 500
        return JSONResponse(
            status_code=200,
            content={
                "use_chatgpt": False,
                "has_api_key": False,
                "extraction_available": False,
                "error": str(e)
            }
        )



@router.post("/toggle_chatgpt_extraction")
async def toggle_chatgpt_extraction(enable: bool = Body(...)):
    """
    Active ou d√©sactive l'utilisation de ChatGPT pour l'extraction symbolique.
    """
    try:
        # V√©rifier si la cl√© API est disponible
        if enable and not enhanced_symbolic_memory.openai_api_key:
            return {
                "status": "error",
                "message": "Cl√© API OpenAI non configur√©e. Veuillez d√©finir la variable d'environnement OPENAI_API_KEY.",
                "current_state": False
            }
        
        # Mettre √† jour la configuration
        config.memory.use_chatgpt_for_symbolic_memory = enable
        
        return {
            "status": "success",
            "message": f"Extraction symbolique via ChatGPT {'activ√©e' if enable else 'd√©sactiv√©e'}",
            "current_state": enable
        }
    except Exception as e:
        logger.error(f"Erreur lors de la modification de la configuration: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")



####################################################################################################################################################
########################################################    ROUTES POUR CONFIG DES RULES DU GRAPH SYMBOLIC  ###############################################
##########################################################################################################################################################


# Chemins des fichiers de r√®gles (√† ajouter)
RULES_PATH = os.path.join(config.data_dir, "memories", "symbolic_rules.json")



DEFAULT_RULES = {
    "entity_aliases": {
        "moi": "Ma√´l",
        # autres valeurs par d√©faut
    },
    "entity_types": {
        "chat": "mode_de_communication",
        # autres mappages de types
    },
    "relation_rewrites": {
        "est": "est une instance de",
        # autres r√©√©critures
    }
}

# Charger ou cr√©er les r√®gles
def _load_symbolic_rules():
    logger.info(f"üìÇ Chemin utilis√© pour les r√®gles: {RULES_PATH}")  # ‚úÖ LOG AVANT LE RETURN

    if os.path.exists(RULES_PATH):
        try:
            with open(RULES_PATH, 'r', encoding='utf-8') as f:
                logger.info(f"üìÑ Chargement du fichier de r√®gles...")
                content = json.load(f)
                logger.info(f"üß† R√®gles charg√©es depuis le fichier : {json.dumps(content, indent=2, ensure_ascii=False)}")
                return content
        except Exception as e:
            logger.error(f"Erreur de chargement des r√®gles: {str(e)}")

    # Fallback : cr√©ation fichier si manquant ou invalide
    with open(RULES_PATH, 'w', encoding='utf-8') as f:
        json.dump(DEFAULT_RULES, f, indent=2, ensure_ascii=False)

    logger.warning("‚ö†Ô∏è Fichier de r√®gles cr√©√© par d√©faut (fallback)")
    return DEFAULT_RULES

# Sauvegarder les r√®gles
def _save_symbolic_rules(rules):
    try:
        with open(RULES_PATH, 'w', encoding='utf-8') as f:
            json.dump(rules, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        logger.error(f"Erreur de sauvegarde des r√®gles: {str(e)}")
        return False

# Endpoint pour obtenir les r√®gles
@router.get("/symbolic_rules")
async def get_symbolic_rules():
    """
    R√©cup√®re les r√®gles de post-traitement du graphe symbolique.
    """
    try:
        rules = _load_symbolic_rules()
        return {
            "status": "success",
            "rules": rules
        }
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des r√®gles: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

# Endpoint pour mettre √† jour les r√®gles
@router.put("/symbolic_rules")
async def update_symbolic_rules(rules: Dict[str, Any] = Body(...)):
    """
    Met √† jour les r√®gles de post-traitement du graphe symbolique.
    """
    try:
        success = _save_symbolic_rules(rules)
        if success:
            # Notifier le module de m√©moire symbolique pour qu'il recharge les r√®gles
            if hasattr(symbolic_memory, "reload_rules"):
                symbolic_memory.reload_rules()
            
            return {
                "status": "success",
                "message": "R√®gles mises √† jour avec succ√®s"
            }
        else:
            raise HTTPException(status_code=500, detail="√âchec de la sauvegarde des r√®gles")
    except Exception as e:
        logger.error(f"Erreur lors de la mise √† jour des r√®gles: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

# Endpoint pour r√©initialiser les r√®gles
@router.post("/symbolic_rules/reset")
async def reset_symbolic_rules():
    """
    R√©initialise les r√®gles aux valeurs par d√©faut.
    """
    try:
        success = _save_symbolic_rules(DEFAULT_RULES)
        if success:
            # Notifier le module de m√©moire symbolique
            if hasattr(symbolic_memory, "reload_rules"):
                symbolic_memory.reload_rules()
            
            return {
                "status": "success",
                "message": "R√®gles r√©initialis√©es aux valeurs par d√©faut"
            }
        else:
            raise HTTPException(status_code=500, detail="√âchec de la r√©initialisation des r√®gles")
    except Exception as e:
        logger.error(f"Erreur lors de la r√©initialisation des r√®gles: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")


##########################################################################################################################################
# ALLOW POST FORMATAGE DU GRAPH SYMBOLIQUE

entities = symbolic_memory.get_all_entities(include_expired=True)
relations = symbolic_memory.get_all_relations(include_expired=True)
# Avant le formatage final
raw_graph = {
    "entities": {e["entity_id"]: e for e in entities},
    "relations": relations
}

# üîß Post-traitement avec les r√®gles
processed_graph = postprocess_graph(raw_graph)

# Reformater avec D3
if format == "d3":
    result = _format_graph_d3(nx_from_graph(processed_graph))
elif format == "cytoscape":
    result = _format_graph_cytoscape(nx_from_graph(processed_graph))



def nx_from_graph(graph: dict) -> nx.DiGraph:
    G = nx.DiGraph()
    for entity_id, entity in graph["entities"].items():
        G.add_node(entity_id, **{
            "label": entity["name"],
            "type": entity["type"],
            "attributes": entity.get("attributes", {}),
            "confidence": entity.get("confidence", 0.0),
            "group": _get_node_group(entity["type"])
        })
    for rel in graph["relations"]:
        G.add_edge(rel["source"], rel["target"], **{
            "label": rel["relation"],
            "confidence": rel.get("confidence", 0.8),
            "id": f"{rel['source']}_{rel['relation']}_{rel['target']}"
        })
    return G
