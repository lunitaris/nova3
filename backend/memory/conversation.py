import os
import json
import uuid
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import asyncio

from backend.config import config
# Import complet des modules de m√©moire
from backend.memory.synthetic_memory import synthetic_memory
from backend.memory.symbolic_memory import symbolic_memory
from backend.models.model_manager import model_manager
from backend.models.langchain_manager import langchain_manager


from backend.memory.vector_store import vector_store
from backend.memory.symbolic_memory import symbolic_memory
from backend.models.model_manager import model_manager

from backend.memory.automatic_contextualizer import AutomaticMemoryContextualizer

from backend.memory.personal_extractor import ConversationMemoryProcessor






logger = logging.getLogger(__name__)

class Conversation:
    """
    G√®re une conversation avec un utilisateur, incluant l'historique et les m√©tadonn√©es.
    """
    
    def __init__(self, conversation_id: str = None, user_id: str = "anonymous"):
        """
        Initialise ou charge une conversation.
        
        Args:
            conversation_id: ID de la conversation existante √† charger
            user_id: ID de l'utilisateur
        """
        self.conversation_id = conversation_id or str(uuid.uuid4())
        self.user_id = user_id
        self.messages = []
        self.metadata = {
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "title": "Nouvelle conversation",
            "summary": "",
            "topic": "general",
            "tags": []
        }


        # Initialiser le gestionnaire de m√©moire personnelle
        self.memory_processor = ConversationMemoryProcessor(
            model_manager, vector_store, symbolic_memory
        )

        self.memory_contextualizer = AutomaticMemoryContextualizer(
            model_manager, vector_store, symbolic_memory
        )
        
        # Chemin du fichier de conversation
        self.file_path = os.path.join(
            config.data_dir, 
            "conversations", 
            f"{self.conversation_id}.json"
        )
        
        # Si un ID est fourni, tenter de charger la conversation
        if conversation_id:
            self._load_conversation()
        else:
            # Nouvelle conversation, sauvegarder imm√©diatement
            self._save_conversation()
    
    def _load_conversation(self):
        """Charge une conversation existante depuis le stockage."""
        try:
            if os.path.exists(self.file_path):
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.messages = data.get("messages", [])
                    self.metadata = data.get("metadata", self.metadata)
                    self.user_id = data.get("user_id", self.user_id)
                    logger.info(f"Conversation {self.conversation_id} charg√©e avec {len(self.messages)} messages")
            else:
                logger.warning(f"Conversation {self.conversation_id} non trouv√©e, cr√©ation d'une nouvelle")
                self._save_conversation()
        except Exception as e:
            logger.error(f"Erreur lors du chargement de la conversation {self.conversation_id}: {str(e)}")
            self._save_conversation()
    
    def _save_conversation(self):
        """Sauvegarde la conversation dans le stockage."""
        try:
            os.makedirs(os.path.dirname(self.file_path), exist_ok=True)
            
            # Mettre √† jour la date de modification
            self.metadata["updated_at"] = datetime.now().isoformat()
            
            # Pr√©parer les donn√©es
            data = {
                "conversation_id": self.conversation_id,
                "user_id": self.user_id,
                "messages": self.messages,
                "metadata": self.metadata
            }
            
            with open(self.file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"Conversation {self.conversation_id} sauvegard√©e")
        except Exception as e:
            logger.error(f"Erreur lors de la sauvegarde de la conversation {self.conversation_id}: {str(e)}")
    
    def add_message(self, content: str, role: str = "user", metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Ajoute un message √† la conversation.
        
        Args:
            content: Contenu du message
            role: R√¥le de l'exp√©diteur ("user" ou "assistant")
            metadata: M√©tadonn√©es additionnelles du message
            
        Returns:
            Le message ajout√©
        """
        message = {
            "id": str(uuid.uuid4()),
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        self.messages.append(message)
        
        # Limiter la taille de l'historique si n√©cessaire
        max_history = config.memory.max_history_length
        if len(self.messages) > max_history:
            # Avant de tronquer, synth√©tiser la m√©moire des messages anciens
            asyncio.create_task(self._synthesize_old_messages())
            # Garder seulement les messages les plus r√©cents
            self.messages = self.messages[-max_history:]
        
        # Mettre √† jour et sauvegarder
        self.metadata["updated_at"] = datetime.now().isoformat()
        self._save_conversation()
        
        # Si c'est un message utilisateur, mettre √† jour la m√©moire symbolique
        if role == "user":
            asyncio.create_task(self._update_symbolic_memory(content))
        
        return message
    
    async def _update_symbolic_memory(self, content: str):
        """
        Met √† jour la m√©moire symbolique avec le contenu du message.
        
        Args:
            content: Contenu du message
        """
        try:
            # Uniquement traiter les messages suffisamment longs
            if len(content.split()) < 5:
                return
                
            logger.info(f"Mise √† jour de la m√©moire symbolique pour la conversation {self.conversation_id}")
            update_stats = await symbolic_memory.update_graph_from_text(content)
            
            if update_stats.get("entities_added", 0) > 0 or update_stats.get("relations_added", 0) > 0:
                logger.info(f"Graph mis √† jour: {update_stats.get('entities_added', 0)} entit√©s, {update_stats.get('relations_added', 0)} relations")
        except Exception as e:
            logger.error(f"Erreur lors de la mise √† jour de la m√©moire symbolique: {str(e)}")
    
    async def _synthesize_old_messages(self):
        """Synth√©tise les messages anciens avant qu'ils ne soient supprim√©s."""
        try:
            # Prendre les 10 premiers messages ou moins
            old_messages = self.messages[:10]
            if len(old_messages) < 3:  # Pas assez de messages pour synth√©tiser
                return
            
            # Synth√©tiser les messages
            await synthetic_memory.synthesize_conversation(
                old_messages, 
                topic=self.metadata.get("topic", "general")
            )
            
            logger.info(f"Messages anciens de la conversation {self.conversation_id} synth√©tis√©s")
        except Exception as e:
            logger.error(f"Erreur lors de la synth√®se des messages anciens: {str(e)}")
    
    async def generate_title(self) -> str:
        """
        G√©n√®re un titre pour la conversation bas√© sur son contenu.
        
        Returns:
            Titre g√©n√©r√©
        """
        try:
            # V√©rifier s'il y a assez de messages
            if len(self.messages) < 2:
                return "Nouvelle conversation"
            
            # Extraire les premiers messages (3 max)
            sample_messages = self.messages[:3]
            formatted_messages = "\n".join([
                f"{'Utilisateur' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
                for msg in sample_messages
            ])
            
            # Prompt pour g√©n√©rer le titre
            prompt = f"""Voici le d√©but d'une conversation:

{formatted_messages}

G√©n√®re un titre court (5 mots maximum) qui r√©sume le sujet principal de cette conversation.
R√©ponds uniquement avec le titre, sans guillemets ni ponctuation suppl√©mentaire."""
            
            # G√©n√©rer le titre avec un mod√®le l√©ger
            title = await model_manager.generate_response(prompt, complexity="low")
            
            # Nettoyer et limiter la longueur
            title = title.strip().strip('"\'').strip()
            if len(title) > 50:
                title = title[:47] + "..."
            
            # Mettre √† jour et sauvegarder
            self.metadata["title"] = title
            self._save_conversation()
            
            logger.info(f"Titre g√©n√©r√© pour la conversation {self.conversation_id}: {title}")
            return title
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du titre: {str(e)}")
            return "Conversation"
    
    async def generate_summary(self) -> str:
        """
        G√©n√®re un r√©sum√© de la conversation.
        
        Returns:
            R√©sum√© g√©n√©r√©
        """
        try:
            # V√©rifier s'il y a assez de messages
            if len(self.messages) < 3:
                return ""
            
            # Pour les conversations longues, √©chantillonner
            max_messages = 10
            if len(self.messages) > max_messages:
                # Prendre le d√©but, le milieu et la fin
                start = self.messages[:3]
                middle = self.messages[len(self.messages)//2 - 1:len(self.messages)//2 + 2]
                end = self.messages[-3:]
                sample_messages = start + middle + end
            else:
                sample_messages = self.messages
            
            # Formater les messages
            formatted_messages = "\n".join([
                f"{'Utilisateur' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
                for msg in sample_messages
            ])
            
            # Prompt pour g√©n√©rer le r√©sum√©
            prompt = f"""Voici une conversation entre un utilisateur et un assistant:

{formatted_messages}

G√©n√®re un r√©sum√© concis (2-3 phrases) qui capture l'essence de cette conversation.
R√©sum√©:"""
            
            # G√©n√©rer le r√©sum√©
            summary = await model_manager.generate_response(prompt, complexity="medium")
            
            # Mettre √† jour et sauvegarder
            self.metadata["summary"] = summary
            self._save_conversation()
            
            logger.info(f"R√©sum√© g√©n√©r√© pour la conversation {self.conversation_id}")
            return summary
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du r√©sum√©: {str(e)}")
            return ""
    
    def clear_history(self):
        """Efface l'historique de la conversation tout en conservant les m√©tadonn√©es."""
        try:
            # Sauvegarder la synth√®se avant de supprimer
            if len(self.messages) > 0:
                asyncio.create_task(synthetic_memory.synthesize_conversation(
                    self.messages,
                    topic=self.metadata.get("topic", "general")
                ))
            
            self.messages = []
            self.metadata["updated_at"] = datetime.now().isoformat()
            self._save_conversation()
            
            logger.info(f"Historique de la conversation {self.conversation_id} effac√©")
        except Exception as e:
            logger.error(f"Erreur lors de l'effacement de l'historique: {str(e)}")
    
    def get_messages(self, limit: int = None, include_metadata: bool = False) -> List[Dict[str, Any]]:
        """
        R√©cup√®re les messages de la conversation.
        
        Args:
            limit: Nombre maximal de messages √† retourner
            include_metadata: Si True, inclut les m√©tadonn√©es
            
        Returns:
            Liste des messages
        """
        messages = self.messages
        
        if limit:
            messages = messages[-limit:]
        
        if not include_metadata:
            # Filtrer les m√©tadonn√©es des messages
            messages = [{k: v for k, v in msg.items() if k != 'metadata'} for msg in messages]
        
        return messages
    
    def get_context_for_model(self, max_messages: int = None) -> str:
        """
        Pr√©pare le contexte de conversation pour le mod√®le.
        
        Args:
            max_messages: Nombre maximal de messages √† inclure
            
        Returns:
            Contexte format√© pour le mod√®le
        """
        # Limiter le nombre de messages si sp√©cifi√©
        messages = self.messages
        if max_messages:
            messages = messages[-max_messages:]
        
        # Formater pour le mod√®le
        formatted_messages = []
        for msg in messages:
            prefix = "Utilisateur: " if msg["role"] == "user" else "Assistant: "
            formatted_messages.append(f"{prefix}{msg['content']}")
        
        return "\n".join(formatted_messages)

class ConversationManager:
    """
    G√®re toutes les conversations et fournit des m√©thodes pour y acc√©der.
    """
    
    def __init__(self):
        """Initialise le gestionnaire de conversations."""
        self.conversations = {}  # Cache des conversations actives
        self.conversations_dir = os.path.join(config.data_dir, "conversations")
        os.makedirs(self.conversations_dir, exist_ok=True)
        
        # C'EST ICI QU'IL FAUT AJOUTER LE CODE :
        # Initialiser le gestionnaire de m√©moire personnelle
        self.memory_processor = ConversationMemoryProcessor(
            model_manager, vector_store, symbolic_memory
        )


        self.memory_contextualizer = AutomaticMemoryContextualizer(
            model_manager, vector_store, symbolic_memory
        )

    
        # Initialiser le contextualiseur automatique
        self.memory_contextualizer = AutomaticMemoryContextualizer(
            model_manager, vector_store, symbolic_memory
        )
    


    def get_conversation(self, conversation_id: str = None, user_id: str = "anonymous") -> Conversation:
        """
        R√©cup√®re ou cr√©e une conversation.
        
        Args:
            conversation_id: ID de la conversation √† r√©cup√©rer
            user_id: ID de l'utilisateur
            
        Returns:
            Instance de Conversation
        """
        # Si pas d'ID sp√©cifi√©, cr√©er une nouvelle conversation
        if not conversation_id:
            conversation = Conversation(user_id=user_id)
            self.conversations[conversation.conversation_id] = conversation
            return conversation
        
        # V√©rifier si la conversation est d√©j√† en cache
        if conversation_id in self.conversations:
            return self.conversations[conversation_id]
        
        # Charger la conversation
        conversation = Conversation(conversation_id=conversation_id, user_id=user_id)
        self.conversations[conversation_id] = conversation
        
        return conversation
    
    def list_conversations(self, user_id: str = None, limit: int = 10, offset: int = 0) -> List[Dict[str, Any]]:
        """
        Liste les conversations disponibles.
        
        Args:
            user_id: Filtre optionnel par utilisateur
            limit: Nombre maximal de conversations √† retourner
            offset: Index de d√©part pour la pagination
            
        Returns:
            Liste des m√©tadonn√©es de conversation
        """
        try:
            # Lister tous les fichiers de conversation
            conversation_files = []
            for filename in os.listdir(self.conversations_dir):
                if filename.endswith(".json"):
                    file_path = os.path.join(self.conversations_dir, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            
                            # Filtrer par utilisateur si sp√©cifi√©
                            if user_id and data.get("user_id") != user_id:
                                continue
                            
                            # Extraire les m√©tadonn√©es principales
                            conversation_info = {
                                "conversation_id": data.get("conversation_id"),
                                "title": data.get("metadata", {}).get("title", "Conversation"),
                                "last_updated": data.get("metadata", {}).get("updated_at"),
                                "summary": data.get("metadata", {}).get("summary", ""),
                                "message_count": len(data.get("messages", [])),
                                "topics": data.get("metadata", {}).get("tags", [])
                            }
                            
                            conversation_files.append(conversation_info)
                    except Exception as e:
                        logger.error(f"Erreur lors de la lecture du fichier {filename}: {str(e)}")
            
            # Trier par date de mise √† jour (plus r√©cent d'abord)
            conversation_files.sort(
                key=lambda x: x.get("last_updated", ""),
                reverse=True
            )
            
            # Appliquer pagination
            paginated_results = conversation_files[offset:offset+limit]
            
            return paginated_results
            
        except Exception as e:
            logger.error(f"Erreur lors de la liste des conversations: {str(e)}")
            return []
    
    def delete_conversation(self, conversation_id: str) -> bool:
        """
        Supprime une conversation.
        
        Args:
            conversation_id: ID de la conversation √† supprimer
            
        Returns:
            True si supprim√©e avec succ√®s, False sinon
        """
        try:
            file_path = os.path.join(self.conversations_dir, f"{conversation_id}.json")
            
            if os.path.exists(file_path):
                os.remove(file_path)
                
                # Supprimer du cache si pr√©sente
                if conversation_id in self.conversations:
                    del self.conversations[conversation_id]
                
                logger.info(f"Conversation {conversation_id} supprim√©e")
                return True
            else:
                logger.warning(f"Conversation {conversation_id} non trouv√©e pour suppression")
                return False
                
        except Exception as e:
            logger.error(f"Erreur lors de la suppression de la conversation {conversation_id}: {str(e)}")
            return False
    
    async def process_user_input(self, conversation_id: str, user_input: str, user_id: str = "anonymous", mode: str = "chat", websocket = None ) -> Dict[str, Any]:
        """
        Traite l'entr√©e utilisateur et g√©n√®re une r√©ponse.
        
        Args:
            conversation_id: ID de la conversation
            user_input: Texte de l'entr√©e utilisateur
            user_id: ID de l'utilisateur
            mode: Mode de conversation ("chat" ou "voice")
            websocket: WebSocket optionnel pour streaming
            
        Returns:
            R√©ponse g√©n√©r√©e avec m√©tadonn√©es
        """
        try:
            # R√©cup√©rer ou cr√©er la conversation
            conversation = self.get_conversation(conversation_id, user_id)

            # Traiter la m√©moire personnelle
            memory_results = await self.memory_processor.process_conversation_message(user_input, user_id)
            
            # Ajouter le message utilisateur
            conversation.add_message(user_input, role="user", metadata={"mode": mode})

            # R√©cup√©rer le contexte personnel pour enrichir le contexte
            personal_context = await self.context_retriever.get_relevant_context(user_input, user_id)
            
            # V√©rifier si c'est une demande de m√©morisation explicite
            if user_input.lower().startswith("souviens-toi") or \
               user_input.lower().startswith("rappelle-toi") or \
               user_input.lower().startswith("m√©morise"):
                # Extraire l'information √† m√©moriser
                info_to_memorize = user_input.split(" ", 1)[1] if " " in user_input else ""
                
                if info_to_memorize:
                    # Stocker dans la m√©moire explicite
                    memory_id = synthetic_memory.remember_explicit_info(info_to_memorize)
                    
                    # Mettre √©galement √† jour la m√©moire symbolique
                    asyncio.create_task(symbolic_memory.update_graph_from_text(info_to_memorize))
                    
                    # R√©ponse de confirmation
                    response_text = f"üåÄ J'ai m√©moris√© cette information : \"{info_to_memorize}\""
                else:
                    response_text = "‚ö°Ô∏è Je n'ai pas compris ce que je dois m√©moriser. Pourriez-vous reformuler?"
            
            else:
                # Utiliser LangChain pour g√©n√©rer la r√©ponse et Ajouter le contexte personnel au contexte de conversation
                response_text = await langchain_manager.process_message(
                    message=user_input,
                    conversation_history=conversation.get_messages(max_messages=10),
                    websocket=websocket,
                    mode=mode,
                    additional_context=enriched_context  # ‚Üê cl√© ici
                )

            # D√©terminer si l'assistant doit mentionner qu'il a m√©moris√© quelque chose
            should_acknowledge = await self.memory_processor.should_acknowledge_memory(memory_results)
            
            if should_acknowledge:
                acknowledgment = self.memory_processor.get_memory_acknowledgment(memory_results)
                if acknowledgment:
                    response_text = f"{response_text} {acknowledgment}"
        

            # Ajouter la r√©ponse √† la conversation
            conversation.add_message(response_text, role="assistant", metadata={"mode": mode})
            
            # Si c'est une nouvelle conversation ou peu de messages, g√©n√©rer un titre
            if len(conversation.messages) <= 4 and not conversation.metadata.get("title"):
                asyncio.create_task(conversation.generate_title())
            
            # Pr√©parer la r√©ponse
            response = {
                "response": response_text,
                "conversation_id": conversation.conversation_id,
                "timestamp": datetime.now().isoformat(),
                "mode": mode,
                # Ajouter les statistiques de m√©morisation aux m√©tadonn√©es
                "memory_stats": memory_results
            }
            
            return response
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement de l'entr√©e utilisateur: {str(e)}")
            
            # R√©ponse d'erreur
            error_response = {
                "response": "Je rencontre une difficult√© √† traiter votre demande. Pourriez-vous r√©essayer?",
                "conversation_id": conversation_id,
                "timestamp": datetime.now().isoformat(),
                "mode": mode,
                "error": str(e)
            }
            
            return error_response

# Instance globale du gestionnaire de conversations
conversation_manager = ConversationManager()