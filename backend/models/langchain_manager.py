
"""
Gestionnaire LangChain pour l'assistant IA.
Orchestre les cha√Ænes de prompts et les m√©moires pour les conversations.
"""
import os
import logging
from typing import Dict, Any, List, Optional, Union
import asyncio

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

from backend.models.model_manager import model_manager
from backend.models.skills.manager import skills_manager
from backend.config import config

from backend.utils.profiler import profile
from backend.utils.startup_log import add_startup_event





logger = logging.getLogger(__name__)

class LangChainManager:
    """
    Gestionnaire des cha√Ænes LangChain, int√©grant les diff√©rents types de m√©moire
    et permettant des conversations contextuelles avanc√©es.
    """
    
    def __init__(self):
        """Initialise le gestionnaire de cha√Ænes LangChain."""
        self.system_prompt = """Tu es une intelligence artificielle locale appel√©e Nova, con√ßue pour assister Ma√´l dans son quotidien.

Voici ce que tu sais sur Ma√´l, ton utilisateur :
{context}

Voici tes instructions :
- R√©ponds directement √† ce qu‚Äôon te demande, sans reformuler ce que Ma√´l a dit.
- Ne redis pas ce que tu sais d√©j√†, sauf si on te le demande.
- Sois concise, pertinente et naturelle, comme une IA vocale intelligente.
- Si tu ne sais pas, dis-le simplement, sans inventer.
- Tu peux te baser sur le contexte symbolique pour enrichir ta r√©ponse intelligemment.
"""
        self._init_memory_chains()
    
    def _init_memory_chains(self):
        """Initialise les cha√Ænes de m√©moire."""
        # logger.info("Initialisation des cha√Ænes de m√©moire")  ## DEBUG
        add_startup_event({"icon": "üß†", "label": "LangChain", "message": "cha√Ænes conversationnelles pr√™tes"})
    

    def _format_conversation_history(self, messages: List[Dict[str, Any]]) -> List[Union[HumanMessage, AIMessage]]:
        """
        Formate l'historique de conversation pour LangChain.
        
        Args:
            messages: Liste des messages de la conversation
            
        Returns:
            Liste format√©e de messages LangChain
        """
        formatted_messages = []
        
        for msg in messages:
            if msg["role"] == "user":
                formatted_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                formatted_messages.append(AIMessage(content=msg["content"]))
        
        return formatted_messages


    async def process_message( self, message: str, conversation_history: List[Dict[str, Any]], websocket=None, mode: str = "chat", additional_context: str = "") -> str:
        """
        Traite un message utilisateur et g√©n√®re une r√©ponse avec LangChain.
        
        Args:
            message: Le message ou la requ√™te de l'utilisateur
            conversation_history: Historique r√©cent de la conversation
            websocket: WebSocket optionnel pour le streaming
            mode: Mode de conversation ('chat' ou 'voice')
            additional_context: Contexte personnel additionnel √† int√©grer
            
        Returns:
            La r√©ponse g√©n√©r√©e
        """
        try:


            context = additional_context
            formatted_history = self._format_conversation_history(conversation_history)
            complexity = "low" if mode == "voice" or len(message.split()) < 10 else "medium"

            # 6. Construire le prompt
            prompt = ChatPromptTemplate.from_messages([
                ("system", self.system_prompt.replace("{context}", context)),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}")
            ])
            
            # 7. Utiliser le mod√®le pour obtenir une r√©ponse
            chain = (
                {"input": RunnablePassthrough(), "chat_history": lambda _: formatted_history}
                | prompt
                | RunnableLambda(lambda x: model_manager.generate_response(
                    x,
                    websocket=websocket,
                    complexity=complexity
                ))
            )
            
            # Ex√©cuter la cha√Æne de fa√ßon asynchrone
            response = await chain.ainvoke(message)
            
            return response
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement du message: {str(e)}")
            return "Je suis d√©sol√©, j'ai rencontr√© une erreur lors du traitement de votre demande. Pourriez-vous reformuler ou r√©essayer plus tard?"
##############


# Instance globale du gestionnaire LangChain
langchain_manager = LangChainManager()