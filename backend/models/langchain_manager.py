
"""
Gestionnaire LangChain pour l'assistant IA.
Orchestre les cha√Ænes de prompts et les m√©moires pour les conversations.
"""
import os
import logging
from typing import Dict, Any, List, Optional, Union
import asyncio

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

from backend.models.model_manager import model_manager
from backend.memory.vector_store import vector_store
from backend.memory.synthetic_memory import synthetic_memory
from backend.memory.symbolic_memory import symbolic_memory
from backend.models.skills.manager import skills_manager
from backend.config import config

from backend.utils.profiler import profile
from backend.utils.startup_log import add_startup_event





logger = logging.getLogger(__name__)

class LangChainManager:
    """
    Gestionnaire des cha√Ænes LangChain, int√©grant les diff√©rents types de m√©moire
    et permettant des conversations contextuelles avanc√©es.
    """
    
    def __init__(self):
        """Initialise le gestionnaire de cha√Ænes LangChain."""
        self.system_prompt = """Tu es un assistant IA local nomm√© Assistant. Tu es serviable, poli et informatif.
Voici quelques r√®gles que tu dois suivre:
- R√©ponds de mani√®re concise et pr√©cise aux questions.
- Si tu ne connais pas la r√©ponse, dis simplement que tu ne sais pas.
- Utilise un langage simple et accessible, sauf si on te demande d'√™tre plus technique.
- N'invente pas d'informations.
- Respecte les pr√©f√©rences de l'utilisateur.
- Pour les commandes domotiques et les actions sp√©cifiques, sois pr√©cis dans ta compr√©hension.

{context}
"""
        self._init_memory_chains()
    
    def _init_memory_chains(self):
        """Initialise les cha√Ænes de m√©moire."""
        # logger.info("Initialisation des cha√Ænes de m√©moire")  ## DEBUG
        add_startup_event({"icon": "üß†", "label": "LangChain", "message": "cha√Ænes conversationnelles pr√™tes"})
    
    async def _get_relevant_context(self, query: str, conversation_history: List[Dict[str, Any]]) -> str:
        """
        R√©cup√®re le contexte pertinent √† partir des m√©moires.
        
        Args:
            query: La requ√™te ou le message actuel de l'utilisateur
            conversation_history: L'historique r√©cent de la conversation
            
        Returns:
            Contexte format√© pour le prompt
        """
        try:
            # R√©cup√©rer les souvenirs pertinents
            vector_memories = vector_store.search_memories(query, k=3)
            
            # R√©cup√©rer les souvenirs synth√©tiques
            synthetic_memories = synthetic_memory.get_relevant_memories(query, max_results=2)
            
            # Formater le contexte
            context = "Informations issues de la m√©moire:\n"
            
            if vector_memories:
                context += "\nM√©moire explicite:\n"
                for i, memory in enumerate(vector_memories, 1):
                    content = memory.get("content", "")
                    timestamp = memory.get("timestamp", "")
                    context += f"{i}. {content} (M√©moris√© le: {timestamp})\n"
            
            if synthetic_memories:
                context += "\nM√©moire synth√©tique:\n"
                for i, memory in enumerate(synthetic_memories, 1):
                    content = memory.get("content", "")
                    context += f"{i}. {content}\n"
            
            if not vector_memories and not synthetic_memories:
                context += "Aucune information pertinente en m√©moire pour cette requ√™te.\n"
            
            return context
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration du contexte m√©moriel: {str(e)}")
            return "Erreur d'acc√®s √† la m√©moire."
    
    def _format_conversation_history(self, messages: List[Dict[str, Any]]) -> List[Union[HumanMessage, AIMessage]]:
        """
        Formate l'historique de conversation pour LangChain.
        
        Args:
            messages: Liste des messages de la conversation
            
        Returns:
            Liste format√©e de messages LangChain
        """
        formatted_messages = []
        
        for msg in messages:
            if msg["role"] == "user":
                formatted_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                formatted_messages.append(AIMessage(content=msg["content"]))
        
        return formatted_messages
    
    async def _detect_intent(self, query: str) -> Dict[str, Any]:
        """
        D√©tecte l'intention de l'utilisateur pour des commandes sp√©cifiques.
        
        Args:
            query: La requ√™te de l'utilisateur
            
        Returns:
            Informations sur l'intention d√©tect√©e
        """
        # Sch√©ma simple pour d√©tecter les commandes domotiques, les rappels, etc.
        intent_prompt = """Analyse la requ√™te utilisateur et identifie si c'est une commande sp√©cifique.
Retourne une r√©ponse au format JSON avec:
- "intent": le type de commande ("domotique", "rappel", "recherche", "general", etc.)
- "confidence": niveau de confiance entre 0 et 1
- "entities": entit√©s extraites de la requ√™te (appareils, lieux, dates, etc.)

Requ√™te: {query}

Format de r√©ponse (JSON):
"""
        
        try:
            # Utiliser un mod√®le rapide pour la d√©tection d'intention
            response = await model_manager.generate_response(
                intent_prompt.replace("{query}", query),
                complexity="low"
            )
            
            # Extraire la partie JSON
            import json
            try:
                # Supprimer les backticks et identifiants de format potentiels
                clean_response = response.replace("```json", "").replace("```", "").strip()
                intent_data = json.loads(clean_response)
                return intent_data
            except json.JSONDecodeError:
                logger.warning(f"R√©ponse d'intention non parsable: {response}")
                return {
                    "intent": "general",
                    "confidence": 0.5,
                    "entities": {}
                }
                
        except Exception as e:
            logger.error(f"Erreur lors de la d√©tection d'intention: {str(e)}")
            # Valeur par d√©faut
            return {
                "intent": "general",
                "confidence": 0.3,
                "entities": {}
            }
####################################################
    #
    #


    async def process_message( self, message: str, conversation_history: List[Dict[str, Any]], websocket=None, mode: str = "chat", additional_context: str = "") -> str:
        """
        Traite un message utilisateur et g√©n√®re une r√©ponse avec LangChain.
        
        Args:
            message: Le message ou la requ√™te de l'utilisateur
            conversation_history: Historique r√©cent de la conversation
            websocket: WebSocket optionnel pour le streaming
            mode: Mode de conversation ('chat' ou 'voice')
            additional_context: Contexte personnel additionnel √† int√©grer
            
        Returns:
            La r√©ponse g√©n√©r√©e
        """
        try:
            # 1. D√©tecter l'intention (pour traiter diff√©remment selon le type de requ√™te)
            intent_data = await self._detect_intent(message)
            logger.info(f"Intention d√©tect√©e: {intent_data['intent']} (confiance: {intent_data['confidence']})")
            
            # 2. R√©cup√©rer le contexte depuis les m√©moires
            context = await self._get_relevant_context(message, conversation_history)
            
            # 3. Int√©grer le contexte personnel s'il existe
            if additional_context:
                # Ajouter le contexte personnel en d√©but de contexte pour lui donner plus d'importance
                context = f"{additional_context}\n\n{context}"
            
            # 4. D√©terminer la complexit√© requise selon l'intention et la longueur
            if mode == "voice" or intent_data["intent"] == "general":
                complexity = "low" if len(message.split()) < 10 else "medium"
            else:
                complexity = "medium"
            
            # 5. Formater l'historique et le contexte
            formatted_history = self._format_conversation_history(conversation_history[-5:])  # Limiter √† 5 derniers messages
            
            # 6. Construire le prompt
            prompt = ChatPromptTemplate.from_messages([
                ("system", self.system_prompt.replace("{context}", context)),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}")
            ])
            
            # 7. Utiliser le mod√®le pour obtenir une r√©ponse
            chain = (
                {"input": RunnablePassthrough(), "chat_history": lambda _: formatted_history}
                | prompt
                | RunnableLambda(lambda x: model_manager.generate_response(
                    x,
                    websocket=websocket,
                    complexity=complexity
                ))
            )
            
            # Ex√©cuter la cha√Æne de fa√ßon asynchrone
            response = await chain.ainvoke(message)
            
            return response
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement du message: {str(e)}")
            return "Je suis d√©sol√©, j'ai rencontr√© une erreur lors du traitement de votre demande. Pourriez-vous reformuler ou r√©essayer plus tard?"
##############
    async def _get_relevant_context(self, query: str, conversation_history: List[Dict[str, Any]]) -> str:
        """
        R√©cup√®re le contexte pertinent √† partir des m√©moires.
        
        Args:
            query: La requ√™te ou le message actuel de l'utilisateur
            conversation_history: L'historique r√©cent de la conversation
            
        Returns:
            Contexte format√© pour le prompt
        """
        try:
            # R√©cup√©rer les souvenirs pertinents
            vector_memories = vector_store.search_memories(query, k=3)
            
            # R√©cup√©rer les souvenirs synth√©tiques
            synthetic_memories = synthetic_memory.get_relevant_memories(query, max_results=2)
            
            # R√©cup√©rer le contexte du graphe symbolique
            symbolic_context = symbolic_memory.get_context_for_query(query, max_results=2)
            
            # Formater le contexte
            context = "Informations issues de la m√©moire:\n"
            has_content = False
            
            if vector_memories:
                context += "\nM√©moire explicite:\n"
                for i, memory in enumerate(vector_memories, 1):
                    content = memory.get("content", "")
                    timestamp = memory.get("timestamp", "")
                    context += f"{i}. {content} (M√©moris√© le: {timestamp})\n"
                has_content = True
            
            if synthetic_memories:
                context += "\nM√©moire synth√©tique:\n"
                for i, memory in enumerate(synthetic_memories, 1):
                    content = memory.get("content", "")
                    context += f"{i}. {content}\n"
                has_content = True
            
            if symbolic_context:
                context += "\nM√©moire symbolique:\n"
                context += symbolic_context + "\n"
                has_content = True
            
            if not has_content:
                context += "Aucune information pertinente en m√©moire pour cette requ√™te.\n"
            
            return context
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration du contexte m√©moriel: {str(e)}")
            return "Erreur d'acc√®s √† la m√©moire."



# Instance globale du gestionnaire LangChain
langchain_manager = LangChainManager()