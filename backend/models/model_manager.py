"""
Gestionnaire de mod√®les LLM pour l'assistant IA.
"""
import logging
import time
import os
from typing import Dict, Any, Optional
import asyncio

from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI

from backend.config import config
from backend.config import OPENAI_API_KEY
from langchain_core.callbacks.base import BaseCallbackHandler
from backend.utils.profiler import profile
from backend.utils.startup_log import add_startup_event
import textwrap
from backend.models.streaming_handler import StreamingWebSocketCallbackHandler
import asyncio  # <-- Ajout important !
from typing import Dict, Any, Optional




logger = logging.getLogger(__name__)

class StreamingWebSocketCallbackHandler(BaseCallbackHandler):
    """Callback handler pour le streaming vers WebSocket."""
    
    def __init__(self, websocket=None):
        """Initialise le handler avec un WebSocket optionnel."""
        self.websocket = websocket
        self.is_active = True
        self.sending = False  # ‚Üê AJOUTE CETTE VARIABLE POUR CASSER LA BOUCLE
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Appel√© √† chaque nouveau token g√©n√©r√© par le LLM."""
        if not self.websocket or not self.is_active:
            return
            
        try:
            # La solution la plus simple : stocker les tokens et laisser le message de fin les envoyer
            # Cette approche contourne le probl√®me de boucle d'√©v√©nements
            
            # Logger le token pour le d√©bogage
            logger.debug(f"Token g√©n√©r√©: '{token}'")
            
            # Envoyer le token de fa√ßon synchrone (solution de contournement)
            try:
                import asyncio
                loop = asyncio.get_event_loop()
            except RuntimeError:
                # Si pas de boucle d'√©v√©nements, en cr√©er une nouvelle
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # Utiliser une approche qui ne bloque pas
            if not loop.is_running():
                # Ex√©cuter directement si la boucle n'est pas en cours d'ex√©cution
                fut = asyncio.run_coroutine_threadsafe(self._send_token(token), loop)
                # Attendre avec un court timeout
                try:
                    fut.result(timeout=0.1)
                except:
                    pass  # Ignorer les timeouts
            else:
                # Si la boucle est en cours d'ex√©cution, planifier pour plus tard
                loop.call_soon_threadsafe(lambda: asyncio.create_task(self._send_token(token)))
                
        except Exception as e:
            self.is_active = False
            logger.error(f"Impossible d'envoyer un token via WebSocket: {str(e)}")
            # Ne pas lever l'exception pour ne pas interrompre la g√©n√©ration


    async def _send_token(self, token: str):
        """Envoie un token via WebSocket."""
        try:
            await self.websocket.send_json({
                "type": "token",
                "content": token
            })
            logger.debug(f"Token envoy√©: {token}")
        except Exception as e:
            # Si l'envoi √©choue, d√©sactiver ce handler
            self.is_active = False
            logger.error(f"√âchec d'envoi de token: {str(e)}")



class ModelManager:
    """
    G√®re les diff√©rents mod√®les LLM et s√©lectionne le plus appropri√© selon le contexte.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(ModelManager, cls).__new__(cls)
        return cls._instance

    
    def __init__(self):
        """Initialise le gestionnaire de mod√®les."""
        self.models = {}
        self._initialize_models()
        
    def _initialize_models(self):
        llm_models = []
        try:
            for model_id, model_config in config.models.items():
                if model_config.type == "local":
                    try:
                        self.models[model_id] = self._init_ollama_model(model_config)
                        # logger.info(f"Mod√®le local {model_id} ({model_config.name}) initialis√©")  ## DEBUG
                        llm_models.append(f"{model_config.name} ({model_id})")
                    except Exception as e:
                        logger.error(f"Erreur d'initialisation du mod√®le {model_id}: {str(e)}")

                elif model_config.type == "cloud" and model_config.name.startswith("gpt"):
                    try:
                        if not OPENAI_API_KEY:
                            logger.warning(f"Cl√© API OpenAI manquante pour le mod√®le {model_id}")
                        else:
                            # logger.info(f"Mod√®le cloud {model_config.name} ({model_id}) configur√©")   ## DEBUG
                            llm_models.append(f"{model_config.name} ({model_id}, cloud)")
                    except Exception as e:
                        logger.error(f"Erreur de validation du mod√®le cloud {model_id}: {str(e)}")

            if llm_models:
                models_str = ", ".join(llm_models)
                add_startup_event({
                    "icon": "üß†",
                    "label": "Mod√®les LLM",
                    "message": models_str
                })

        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation des mod√®les: {str(e)}")


    # @profile("ollama_init")       ## DEBUG A DECOMMENTER SI BESOIN
    def _init_ollama_model(self, model_config):
        """
        Initialise un mod√®le Ollama.
        
        Args:
            model_config: Configuration du mod√®le
            
        Returns:
            Instance Ollama configur√©e
        """
        return OllamaLLM(
            model=model_config.name,
            temperature=model_config.parameters.get("temperature", 0.7),
            top_p=model_config.parameters.get("top_p", 0.9),
            top_k=model_config.parameters.get("top_k", 40),
            num_ctx=model_config.context_window,
            base_url=model_config.api_base,
            callbacks=[]  # FIX: Retirez les callbacks de d√©marrage qui pourraient causer des probl√®mes
        )


    @profile("llm_get_model")
    def _get_appropriate_model(self, prompt: str, complexity: str = "auto", websocket=None):
        """
        S√©lectionne le mod√®le le plus appropri√© selon le contexte.
        
        Args:
            prompt: Prompt √† traiter
            complexity: Complexit√© de la requ√™te ("low", "medium", "high" ou "auto")
            websocket: WebSocket pour streaming (optionnel)
            
        Returns:
            Instance du mod√®le s√©lectionn√©
        """
        # Si mod√®le sp√©cifi√© directement, l'utiliser si disponible
        if complexity in self.models:
            model_id = complexity
        
        # Sinon, s√©lection automatique selon longueur et complexit√©
        else:
            # Estimation simple de la longueur de la r√©ponse attendue
            prompt_length = len(prompt.split())
            
            if complexity == "auto":
                # Logique simple d'estimation de complexit√©
                if prompt_length < 100:
                    model_id = "fast"  # Requ√™tes courtes
                else:
                    model_id = "balanced"  # Requ√™tes moyennes √† longues
            elif complexity == "low":
                model_id = "fast"
            elif complexity == "high" and "cloud_fallback" in self.models:
                model_id = "cloud_fallback"
            else:
                model_id = "balanced"  # Par d√©faut pour complexit√© moyenne
        
        
        # Si le mod√®le n'est pas disponible, fallback
        if model_id not in self.models:
            available_models = list(self.models.keys())
            if not available_models:
                raise ValueError("Aucun mod√®le disponible")
            
            logger.info(f"[LLM] 1.....Mod√®le s√©lectionn√© : {model_id}")
            # Utiliser le premier mod√®le disponible
            model_id = available_models[0]        

        logger.info(f"[ModelManager] Mod√®le s√©lectionn√© : {model_id}")
        # Pour OpenAI, cr√©er une nouvelle instance √† chaque fois avec streaming si n√©cessaire
        if model_id == "cloud_fallback":
            model_config = config.models[model_id]
            
            callbacks = []
            if websocket:
                callbacks.append(StreamingWebSocketCallbackHandler(websocket))
            
            return ChatOpenAI(
                model_name=model_config.name,
                temperature=model_config.parameters.get("temperature", 0.7),
                max_tokens=model_config.parameters.get("max_tokens", 1024),
                streaming=bool(websocket),
                callbacks=callbacks  # Utiliser callbacks directement
            )
        
        # Pour les mod√®les locaux
        model = self.models[model_id]
        
        # Si WebSocket fourni et streaming demand√©, configurer le callback
        if websocket and hasattr(model, 'callbacks'):               # Nouvelle fa√ßon (si le mod√®le supporte les callbacks dynamiques):
            model.callbacks.append(StreamingWebSocketCallbackHandler(websocket))
        return model
    
    @profile("generate_response")
    async def generate_response(self, prompt: str, websocket=None, complexity: str = "auto",max_retries: int = 2,retry_delay: int = 1,caller: str = "unknown") -> str:
        """
        G√©n√®re une r√©ponse √† partir du prompt.
        
        Args:
            prompt: Prompt pour la g√©n√©ration
            websocket: WebSocket pour streaming (optionnel)
            complexity: Complexit√© de la requ√™te ("low", "medium", "high" ou "auto")
            max_retries: Nombre maximal de tentatives
            retry_delay: D√©lai entre les tentatives (secondes)
            
        Returns:
            Texte g√©n√©r√©
        """
        logger.debug(f"[DEBUG] Prompt complet envoy√© au LLM:\n{prompt}")
        logger.info(f"[DEBUG] Longueur du prompt : {len(prompt)} caract√®res")
        retries = 0

        logger.info(textwrap.dedent(f"""
        üß† Appel LLM via generate_response()
        ‚Ä¢ Complexit√©       : {complexity}
        ‚Ä¢ Max tokens       : {model_config.parameters.get("max_tokens", "?" ) if 'model_config' in locals() else "?"}
        ‚Ä¢ WebSocket actif  : {"oui" if websocket else "non"}
        ‚Ä¢ Prompt (d√©but)   : {repr(prompt[:120])}...
        """))

        
        while retries <= max_retries:
            try:
                start_time = time.time()
                safe_prompt = prompt[:200].replace('\n', ' ')
                logger.info(f"[ModelManager] ‚û§ Prompt envoy√© (complexit√©={complexity}) : {safe_prompt}...")
                 
                # S√©lectionner le mod√®le appropri√©
                model = self._get_appropriate_model(prompt, complexity, websocket)
                
                # G√©n√©rer la r√©ponse
                logger.info(f"[ModelManager] Prompt word count : {len(prompt.split())}")
                response = await self._generate_from_model(model, prompt, websocket)
                elapsed_time = time.time() - start_time
                logger.info(f"R√©ponse g√©n√©r√©e en {elapsed_time:.2f}s")
                
                # Nettoyer la r√©ponse
                response = response.strip()
                logger.info(f"[ModelManager] ‚úÖ R√©ponse g√©n√©r√©e - taille: {len(response)} caract√®res")
                return response
                
            except Exception as e:
                logger.error(f"Erreur de g√©n√©ration (tentative {retries+1}/{max_retries+1}): {str(e)}")
                retries += 1
                
                if retries <= max_retries:
                    # Attendre avant de r√©essayer
                    await asyncio.sleep(retry_delay)
                else:
                    # Si toutes les tentatives ont √©chou√©
                    error_msg = f"Erreur lors de la g√©n√©ration de r√©ponse apr√®s {max_retries+1} tentatives."
                    logger.error(error_msg)
                    return "D√©sol√©, je rencontre des difficult√©s techniques. Pourriez-vous reformuler ou r√©essayer plus tard?"


    @profile("llm_generation")
    async def _generate_from_model(self, model, prompt, websocket):
        """G√©n√®re la r√©ponse avec le mod√®le sp√©cifi√©."""

        # Si mod√®le OpenAI ‚Üí logique actuelle conserv√©e
        if isinstance(model, ChatOpenAI):
            if websocket:
                from backend.models.streaming_handler import StreamingWebSocketCallbackHandler
                callback = StreamingWebSocketCallbackHandler(websocket)
                response = await model.ainvoke(prompt, config={"callbacks": [callback]})
                # Envoyer les tokens restants √† la fin
                await callback.flush_remaining_tokens()
                return response.content
            else:
                response = await model.ainvoke(prompt)
                return response.content

        # ‚úÖ PATCH OLLAMA STREAMING
        if websocket:
            from backend.models.streaming_handler import StreamingWebSocketCallbackHandler
            callback = StreamingWebSocketCallbackHandler(websocket)
            callback.loop = asyncio.get_running_loop()  # Important: capturer la boucle courante
            
            streamed_chunks = []
            try:
                # Utiliser astream avec le callback
                async for chunk in model.astream(prompt, config={"callbacks": [callback]}):
                    streamed_chunks.append(chunk)
                
                # Vider les tokens restants √† la fin du streaming
                await callback.flush_remaining_tokens()
                
                return "".join(streamed_chunks)
            except Exception as e:
                logger.error(f"Erreur pendant le streaming: {str(e)}")
                # En cas d'erreur, essayer de vider le buffer si possible
                try:
                    await callback.flush_remaining_tokens()
                except:
                    pass
                # Fallback: continuer avec une g√©n√©ration non-streaming
                return await model.ainvoke(prompt)

        # Si pas de WebSocket, r√©ponse normale
        return await model.ainvoke(prompt)


# Instance globale du gestionnaire de mod√®les
model_manager = ModelManager()