"""
Gestionnaire de mod√®les LLM pour l'assistant IA.
"""
import logging
import time
import os
from typing import Dict, Any, Optional
import asyncio

from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI

from backend.config import config
from backend.config import OPENAI_API_KEY
from langchain_core.callbacks.base import BaseCallbackHandler
from backend.utils.profiler import profile
from backend.utils.startup_log import add_startup_event
import textwrap
from backend.models.streaming_callbacks import StreamingWebSocketCallbackHandler
import asyncio  # <-- Ajout important !
from typing import Dict, Any, Optional
from backend.utils.profiler import trace_step, TreeTracer, current_trace  # AJOUT POUR TRACE




logger = logging.getLogger(__name__)

class ModelManager:
    """
    G√®re les diff√©rents mod√®les LLM et s√©lectionne le plus appropri√© selon le contexte.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(ModelManager, cls).__new__(cls)
        return cls._instance

    
    def __init__(self):
        """Initialise le gestionnaire de mod√®les."""
        self.models = {}
        self._initialize_models()
        
    def _initialize_models(self):
        llm_models = []
        try:
            for model_id, model_config in config.models.items():
                if model_config.type == "local":
                    try:
                        self.models[model_id] = self._init_ollama_model(model_config)
                        # logger.info(f"Mod√®le local {model_id} ({model_config.name}) initialis√©")  ## DEBUG
                        llm_models.append(f"{model_config.name} ({model_id})")
                    except Exception as e:
                        logger.error(f"Erreur d'initialisation du mod√®le {model_id}: {str(e)}")

                elif model_config.type == "cloud" and model_config.name.startswith("gpt"):
                    try:
                        if not OPENAI_API_KEY:
                            logger.warning(f"Cl√© API OpenAI manquante pour le mod√®le {model_id}")
                        else:
                            # logger.info(f"Mod√®le cloud {model_config.name} ({model_id}) configur√©")   ## DEBUG
                            llm_models.append(f"{model_config.name} ({model_id}, cloud)")
                    except Exception as e:
                        logger.error(f"Erreur de validation du mod√®le cloud {model_id}: {str(e)}")

            if llm_models:
                models_str = ", ".join(llm_models)
                add_startup_event({
                    "icon": "üß†",
                    "label": "Mod√®les LLM",
                    "message": models_str
                })

        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation des mod√®les: {str(e)}")


    # @profile("ollama_init")       ## DEBUG A DECOMMENTER SI BESOIN
    def _init_ollama_model(self, model_config):
        """
        Initialise un mod√®le Ollama.
        
        Args:
            model_config: Configuration du mod√®le
            
        Returns:
            Instance Ollama configur√©e
        """
        return OllamaLLM(
            model=model_config.name,
            temperature=model_config.parameters.get("temperature", 0.7),
            top_p=model_config.parameters.get("top_p", 0.9),
            top_k=model_config.parameters.get("top_k", 40),
            num_ctx=model_config.context_window,
            base_url=model_config.api_base,
            callbacks=[]  # FIX: Retirez les callbacks de d√©marrage qui pourraient causer des probl√®mes
        )


    @profile("llm_get_model")
    @trace_step("‚öôÔ∏è ModelManager > _get_appropriate_model()")
    def _get_appropriate_model(self, prompt: str, complexity: str = "auto", websocket=None):
        """
        S√©lectionne le mod√®le le plus appropri√© selon le contexte.
        
        Args:
            prompt: Prompt √† traiter
            complexity: Complexit√© de la requ√™te ("low", "medium", "high" ou "auto")
            websocket: WebSocket pour streaming (optionnel)
            
        Returns:
            Instance du mod√®le s√©lectionn√©
        """
        # Si mod√®le sp√©cifi√© directement, l'utiliser si disponible
        if complexity in self.models:
            model_id = complexity
        
        # Sinon, s√©lection automatique selon longueur et complexit√©
        else:
            # Estimation simple de la longueur de la r√©ponse attendue
            prompt_length = len(prompt.split())
            
            if complexity == "auto":
                # Logique simple d'estimation de complexit√©
                if prompt_length < 100:
                    model_id = "fast"  # Requ√™tes courtes
                else:
                    model_id = "balanced"  # Requ√™tes moyennes √† longues
            elif complexity == "low":
                model_id = "fast"
            elif complexity == "high" and "cloud_fallback" in self.models:
                model_id = "cloud_fallback"
            else:
                model_id = "balanced"  # Par d√©faut pour complexit√© moyenne
        
        
        # Si le mod√®le n'est pas disponible, fallback
        if model_id not in self.models:
            available_models = list(self.models.keys())
            if not available_models:
                raise ValueError("Aucun mod√®le disponible")
            
            logger.info(f"[LLM] 1.....Mod√®le s√©lectionn√© : {model_id}")
            # Utiliser le premier mod√®le disponible
            model_id = available_models[0]        

        logger.info(f"[ModelManager] Mod√®le s√©lectionn√© : {model_id}")
        # Pour OpenAI, cr√©er une nouvelle instance √† chaque fois avec streaming si n√©cessaire
        if model_id == "cloud_fallback":
            model_config = config.models[model_id]
            
            callbacks = []
            if websocket:
                callbacks.append(StreamingWebSocketCallbackHandler(websocket))
            
            return ChatOpenAI(
                model_name=model_config.name,
                temperature=model_config.parameters.get("temperature", 0.7),
                max_tokens=model_config.parameters.get("max_tokens", 1024),
                streaming=bool(websocket),
                callbacks=callbacks  # Utiliser callbacks directement
            )
        
        # Pour les mod√®les locaux
        model = self.models[model_id]
        logger.info(f"üß† Mod√®le final utilis√© : {model_id}")
        return model
    
    @profile("generate_response")
    @trace_step("üß† ModelManager > generate_response()")
    async def generate_response(self, prompt: str, websocket=None, complexity: str = "auto",max_retries: int = 2,retry_delay: int = 1,caller: str = "unknown") -> str:
        """
        G√©n√®re une r√©ponse √† partir du prompt.
        
        Args:
            prompt: Prompt pour la g√©n√©ration
            websocket: WebSocket pour streaming (optionnel)
            complexity: Complexit√© de la requ√™te ("low", "medium", "high" ou "auto")
            max_retries: Nombre maximal de tentatives
            retry_delay: D√©lai entre les tentatives (secondes)
            
        Returns:
            Texte g√©n√©r√©
        """
        # ‚ö†Ô∏è OPTIMISATION: logs et surveillance de taille
        safe_preview = prompt[:100].replace("\n", " ") + ("..." if len(prompt) > 100 else "")
        logger.info(f"üß† Appel LLM via generate_response() [{caller}]: complexit√©={complexity}, longueur={len(prompt)}, d√©but={safe_preview}")

        if len(prompt) > 4000:
            logger.warning(f"‚ö†Ô∏è Prompt tr√®s long d√©tect√© ({len(prompt)} caract√®res) ‚Äî √† optimiser")
        logger.debug(f"[DEBUG] Prompt complet envoy√© au LLM:\n{prompt}")
        logger.info(f"[DEBUG] Longueur du prompt : {len(prompt)} caract√®res")
        retries = 0

        logger.info(textwrap.dedent(f"""
        üß† Appel LLM via generate_response()
        ‚Ä¢ Complexit√©       : {complexity}
        ‚Ä¢ Max tokens       : {model_config.parameters.get("max_tokens", "?" ) if 'model_config' in locals() else "?"}
        ‚Ä¢ WebSocket actif  : {"oui" if websocket else "non"}
        ‚Ä¢ Prompt (d√©but)   : {repr(prompt[:120])}...
        """))

        global current_trace
        tracer = TreeTracer("üì§ Envoi prompt au LLM", args={"caller": caller, "complexity": complexity})
        current_trace = tracer

        
        while retries <= max_retries:
            try:
                start_time = time.time()
                safe_prompt = prompt[:200].replace('\n', ' ')
                logger.info(f"[ModelManager] ‚û§ Prompt envoy√© (complexit√©={complexity}) : {safe_prompt}...")
                 
                # S√©lectionner le mod√®le appropri√©
                step_model = tracer.step("üéØ S√©lection du mod√®le")
                model = self._get_appropriate_model(prompt, complexity, websocket)
                step_model.done(type(model).__name__)
                
                # G√©n√©rer la r√©ponse
                logger.info(f"[ModelManager] Prompt word count : {len(prompt.split())}")
                step_gen = tracer.step("‚úçÔ∏è G√©n√©ration de la r√©ponse")
                response = await self._generate_from_model(model, prompt, websocket)
                step_gen.done(f"{len(response.strip())} caract√®res")
                elapsed_time = time.time() - start_time
                logger.info(f"R√©ponse g√©n√©r√©e en {elapsed_time:.2f}s")
                
                # Nettoyer la r√©ponse
                response = response.strip()
                logger.info(f"[ModelManager] ‚úÖ R√©ponse g√©n√©r√©e - taille: {len(response)} caract√®res")
                tracer.done("‚úÖ R√©ponse g√©n√©r√©e")
                return response
                
            except Exception as e:
                logger.error(f"Erreur de g√©n√©ration (tentative {retries+1}/{max_retries+1}): {str(e)}")
                retries += 1
                
                if retries <= max_retries:
                    # Attendre avant de r√©essayer
                    await asyncio.sleep(retry_delay)
                else:
                    # Si toutes les tentatives ont √©chou√©
                    error_msg = f"Erreur lors de la g√©n√©ration de r√©ponse apr√®s {max_retries+1} tentatives."
                    logger.error(error_msg)
                    return "D√©sol√©, je rencontre des difficult√©s techniques. Pourriez-vous reformuler ou r√©essayer plus tard?"


    @profile("llm_generation")
    @trace_step("üß™ ModelManager > _generate_from_model()")
    async def _generate_from_model(self, model, prompt, websocket):
        """G√©n√®re la r√©ponse avec le mod√®le sp√©cifi√©."""

        global current_trace
        tracer = TreeTracer("‚öôÔ∏è Appel du mod√®le", args={"type": type(model).__name__})
        current_trace = tracer

        # Si mod√®le OpenAI ‚Üí logique actuelle conserv√©e
        if isinstance(model, ChatOpenAI):
            if websocket:
                from backend.models.streaming_callbacks import StreamingWebSocketCallbackHandler  # ‚úÖ nouveau chemin
                callback = StreamingWebSocketCallbackHandler(websocket)
                response = await model.ainvoke(prompt, config={"callbacks": [callback]})
                # Envoyer les tokens restants √† la fin
                await callback.flush_remaining_tokens()
                tracer.done("‚úÖ R√©ponse g√©n√©r√©e")
                return response.content
            else:
                response = await model.ainvoke(prompt)
                tracer.done("‚úÖ R√©ponse g√©n√©r√©e")
                return response.content

        # ‚úÖ PATCH OLLAMA STREAMING
        if websocket:
            from backend.models.streaming_callbacks import StreamingWebSocketCallbackHandler  # ‚úÖ nouveau chemin
            callback = StreamingWebSocketCallbackHandler(websocket)            
            streamed_chunks = []
            try:
                # Utiliser astream avec le callback
                async for chunk in model.astream(prompt, config={"callbacks": [callback]}):
                    streamed_chunks.append(chunk)
                
                # Vider les tokens restants √† la fin du streaming
                await callback.flush_remaining_tokens()
                tracer.done("‚úÖ R√©ponse g√©n√©r√©e")
                return "".join(streamed_chunks)
            except Exception as e:
                logger.error(f"Erreur pendant le streaming: {str(e)}")
                # En cas d'erreur, essayer de vider le buffer si possible
                try:
                    await callback.flush_remaining_tokens()
                except:
                    pass
                # Fallback: continuer avec une g√©n√©ration non-streaming
                tracer.done("‚úÖ Fallback g√©n√©ration non streaming..")
                return await model.ainvoke(prompt)

        # Si pas de WebSocket, r√©ponse normale
        return await model.ainvoke(prompt)


# Instance globale du gestionnaire de mod√®les
model_manager = ModelManager()